{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practica2Instrucciones.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IA2021UR/practica-8-joferne/blob/main/Practica8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfaC--RXGK7d"
      },
      "source": [
        "**¡¡¡SIN HACER!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L42gbFh_EJmf"
      },
      "source": [
        "# Práctica 8. Ejercicio adicional 2:  construyendo un detector de objetos\n",
        "\n",
        "En este notebook se muestra cómo crear un modelo de detección de objetos usando la arquitectura Faster R-CNN. Para crear nuestro modelo vamos a utilizar la librería [IceVision](https://airctic.com/) que es una librería para crear modelos de detección usando FastAI.\n",
        "\n",
        "En esta práctica vamos a hacer un uso intensivo de la GPU, así que es importante activar su uso desde la opción Configuración del cuaderno del menú Editar (esta opción debería estar habilitada por defecto, pero es recomendable que lo compruebes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L55Jj3J0EJnV"
      },
      "source": [
        "## Librerías\n",
        "\n",
        "Comenzamos descargando la librería IceVision. Al finalizar la instalación deberás reiniciar el kernel (menú Entorno de ejecución -> Reiniciar Entorno de ejecución)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V43yXELfhMPH"
      },
      "source": [
        "!pip install icevision[all]==0.5.1\n",
        "!pip install torchtext==0.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoMHUXuEJnj"
      },
      "source": [
        "A continuación, cargamos aquellas librerías que son necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q70xWQlAhWkk"
      },
      "source": [
        "from icevision.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrUPWPb9EJnp"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Para esta práctica vamos a usar como ejemplo el [Fruit Images for Object Detection dataset](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection). Este dataset consta de 240 imágenes de entrenamiento y 60 de test con tres categorías: manzanas, plátanos y naranjas. Los siguientes comandos descargan y descomprimen dicho dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlDrFbeOi9aG"
      },
      "source": [
        "!wget https://www.dropbox.com/s/1dsfd5rrmg3riqj/fruits.zip?dl=1 -O fruits.zip\n",
        "!unzip fruits.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWDIq33hjR_B",
        "outputId": "0e265afc-230d-4d91-e319-3eeed1b2769a"
      },
      "source": [
        "Vamos a explorar el contenido de este dataset. Para ello vamos a crear un objeto Path que apunta al directorio que acabamos de crear. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYuhmDGjhrZE"
      },
      "source": [
        "path=Path('fruits')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNGl3HduEJn1"
      },
      "source": [
        "Podemos ver el contenido de este directorio usando el comando `ls()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-RRrBKSny4F"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKpcnPiEJn6"
      },
      "source": [
        "Si exploráis el directorio podréis ver que hay dos carpetas (llamadas train y test), y que cada una de ellas contiene dos carpetas, llamadas images y labels. La carpeta images contiene las imágenes del dataset, y la carpeta labels contiene las anotaciones en formato [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/). Para cada imagen, hay un fichero xml con el mismo nombre que contiene su extensión. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUJJrxQEJn8"
      },
      "source": [
        "## Icevision\n",
        "\n",
        "El proceso para crear y evaluar un modelo de IceVision consta de los siguientes pasos:\n",
        "1. Crear un parser para leer las imágenes y las anotaciones.\n",
        "2. Construir objetos record a partir de los parser. \n",
        "3. Crear los datasets a partir de los records y los aumentos que queramos aplicar. \n",
        "4. Crear un dataloader a partir de los datasets. \n",
        "5. Definir un modelo. \n",
        "6. Entrenar el modelo. \n",
        "7. Guardar el modelo.\n",
        "8. Usar el modelo para inferencia\n",
        "\n",
        "Vamos a ver en detalle cada uno de estos pasos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDSyQqRKEJoC"
      },
      "source": [
        "### Parser\n",
        "\n",
        "IceVision proporciona una serie de parsers definidos por defecto para leer las anotaciones en distintos formatos entre ellos Pascal VOC y COCO. También es posible crear parsers propios. A pesar de que existe un parser para el formato de nuestra anotación, vamos a ver cómo crear un parser desde cero. \n",
        "\n",
        "Un parser ha de heredar de una serie de clases que proporcionan la funcionalidad que necesitamos. En concreto hemos de definir una clase con la siguiente signatura. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvLHn4FhEJoP"
      },
      "source": [
        "class AnotacionParser(parsers.Parser, parsers.FilepathMixin, parsers.LabelsMixin, parsers.BBoxesMixin):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9jYSMKtEJoS"
      },
      "source": [
        "A continuación IceVision proporciona el método `generate_template` que nos proporciona los métodos que debemos implementar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKotjc9eEJoU"
      },
      "source": [
        "AnotacionParser.generate_template()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLlyJGUREJoX"
      },
      "source": [
        "A continuación vamos a implementar nuestra clase con cada uno de esos métodos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSjGOI5PmanQ"
      },
      "source": [
        "# Cargamos la siguiente librería que nos servirá para leer ficheros XML\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "class AnotacionParser(parsers.Parser, parsers.FilepathMixin, parsers.LabelsMixin, parsers.BBoxesMixin):\n",
        "    \"\"\"Definimos el constructor de nuestra clase que va a recibir tres parámetros:\n",
        "       - El path al directorio donde se encuentran las imágenes.\n",
        "       - El path al directorio donde se encuentran las anotaciones.\n",
        "       - Un objeto class_map con las clases que tiene nuestro dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, path_img,path_anotaciones,class_map):\n",
        "        self.path_img = path_img\n",
        "        self.path_anotaciones= path_anotaciones\n",
        "        self.class_map = class_map\n",
        "\n",
        "    \"\"\"El método iter escanea el directorio de anotaciones y nos devuelve el nombre \n",
        "    de cada fichero. Dicho nombre será utilizado por el resto de método\"\"\"    \n",
        "    def __iter__(self):\n",
        "        with os.scandir(self.path_anotaciones) as ficheros:\n",
        "            for fichero in ficheros:\n",
        "                yield fichero.name\n",
        "                \n",
        "    \"\"\"El método prepare recibe el nombre de un fichero de anotación como parámetro y realiza\n",
        "    una serie de labores de preprocesamiento sobre dicho fichero de anotación. En este caso lo procesa\n",
        "    usando la funcionalidad de la librería para trabajar con xml\"\"\"\n",
        "    def prepare(self, o):\n",
        "        tree = ET.parse(str(self.path_anotaciones)+'/'+str(o))\n",
        "        self._root = tree.getroot()\n",
        "\n",
        "    \"\"\"A partir del nombre del fichero de anotación, imageid debe devolver el identificador\n",
        "    (o nombre) de la imagen asociada\"\"\"\n",
        "    def imageid(self, o) -> Hashable: #o --> nombre de la anotación\n",
        "        return o[:o.find('.')]\n",
        "\n",
        "    \"\"\"El método filepath a partir del nombre del fichero de anotación devuelve el path de \n",
        "    la imagen asociada\"\"\"\n",
        "    def filepath(self, o) -> Union[str, Path]:\n",
        "        path=Path(f\"{o[:o.find('.')]}.jpg\")\n",
        "        return self.path_img / path\n",
        "\n",
        "    \"\"\"La función image_width_height devuelve el ancho y el alto de una imagen a partir del nombre\n",
        "    del fichero de anotación\"\"\"\n",
        "    def image_width_height(self, o) -> Tuple[int, int]:\n",
        "        return get_image_size(str(self.path_img)+'/'+f\"{o[:o.find('.')]}.jpg\")\n",
        "\n",
        "    \"\"\"La función labels recibe el nombre del fichero de anotación y debe devolver una lista \n",
        "    con los identificadores de las clases contenidas en dicho fichero.\"\"\"\n",
        "    def labels(self, o) -> List[int]:\n",
        "        labels = []\n",
        "        for object in self._root.iter(\"object\"):\n",
        "            label = object.find(\"name\").text\n",
        "            label_id = self.class_map.get_name(label)\n",
        "            labels.append(label_id)\n",
        "\n",
        "        return labels\n",
        "\n",
        "    \"\"\"La función bboxes recibe el nombre del fichero de anotación y debe devolver una lista \n",
        "    de bboxes que son las anotaciones contenidas en dicho fichero. El formato de cada BBOX es\n",
        "    xmin, ymin, xmax, ymax.\"\"\"\n",
        "    def bboxes(self, o) -> List[BBox]:\n",
        "        def to_int(x):\n",
        "            return int(float(x))\n",
        "\n",
        "        bboxes = []\n",
        "        for object in self._root.iter(\"object\"):\n",
        "            xml_bbox = object.find(\"bndbox\")\n",
        "            xmin = to_int(xml_bbox.find(\"xmin\").text)\n",
        "            ymin = to_int(xml_bbox.find(\"ymin\").text)\n",
        "            xmax = to_int(xml_bbox.find(\"xmax\").text)\n",
        "            ymax = to_int(xml_bbox.find(\"ymax\").text)\n",
        "\n",
        "            bbox = BBox.from_xyxy(xmin, ymin, xmax, ymax)\n",
        "            bboxes.append(bbox)\n",
        "\n",
        "        return bboxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zoXEhtnEJpC"
      },
      "source": [
        "Una vez que hemos definido nuestra clase para parsear las anotaciones de nuestro dataset, vamos a construir los objetos correspondientes. \n",
        "\n",
        "Lo primero que tenemos que hacer es construir nuestro `class_map` que es un objeto de la clase `ClassMap` y que contiene las clases de objetos de nuestro dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25LPb1QdEJpE"
      },
      "source": [
        "class_map = ClassMap(['apple','banana','orange'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAUuna0EEJpE"
      },
      "source": [
        "A continuación definimos nuestros parsers. Uno para leer el conjunto de entrenamiento, y otro para el de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnmVc1hMjjdQ"
      },
      "source": [
        "trainPath = Path('fruits')/'train'\n",
        "parserTrain = AnotacionParser(trainPath/'images', trainPath/'labels', class_map)\n",
        "\n",
        "testPath = Path('fruits')/'test'\n",
        "parserTest = AnotacionParser(testPath/'images', testPath/'labels', class_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5WYgmy7EJpK"
      },
      "source": [
        "### Records\n",
        "\n",
        "Un record es un diccionario que contiene todos los campos parseados definidos en el proceso anterior. Mientras que cada parser es específico para cada anotación concreta, los objetos record tienen una estructura común. Para construir los records a partir de nuestros objetos `parser` debemos llamar al método `parse` e indicarle cómo se van a repartir los datos que se lean. \n",
        "\n",
        "Como siempre, vamos a dividir nuestro dataset en tres partes: un conjunto de entrenamiento, uno de validación y uno de test. Por lo tanto tendremos que construir tres records llamados `train_records`, `valid_records` y `test_records`. Los records de entrenamiento y validación los construiremos a partir de los datos de entrenamiento usando una partición 90/10. Mientras que el record de test se construye a partir del conjunto de test usándolo completamente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyrgs5gJjp1t"
      },
      "source": [
        "train_records, valid_records = parserTrain.parse(RandomSplitter((0.9, 0.1)))\n",
        "test_records,_= parserTest.parse(RandomSplitter((1.0, 0.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT49xefEEJpS"
      },
      "source": [
        "### Transforms\n",
        "\n",
        "Las transformaciones o aumentos son una parte fundamental cuando estamos construyendo modelos en visión por computador. IceVision incluye por defecto la librería [Albumentations](https://github.com/albumentations-team/albumentations) que nos proporciona una gran cantidad de transformaciones. Además es capaz de gestionar los cambios en la anotación que son necesarios cuando se trabaja en detección de objetos. \n",
        "\n",
        "IceVision proporciona una función muy útil que es [`tfms.A.aug_tfms`](https://airctic.com/albumentations_tfms/) con una gran cantidad de transformaciones. Además podemos añadirle cualquier otra transformación de Albumentations. \n",
        "\n",
        "Para nuestro ejemplo vamos a usar las transformaciones sugeridas por defecto por IceVision y aplicar la técnica de presizing vista para clasificación de imágenes; además será necesario normalizar las imágenes al rango de las imágenes de ImageNet. Notar que las transformaciones se aplicarán solo al conjunto de entrenamiento. Para los conjuntos de validación y test únicamente tendremos que escalar la imagen al tamaño adecuado y normalizarla. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gee0nYgk3F3"
      },
      "source": [
        "presize = 512\n",
        "size = 384"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwUimo0Fk-KH"
      },
      "source": [
        "train_tfms = tfms.A.Adapter(\n",
        "    [*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()]\n",
        ")\n",
        "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO8nTtcBEJpV"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "La clase `Dataset` sirve para combinar los records y las transformaciones. Debemos crear un dataset para nuestro conjunto de entrenamiento, otro para el conjunto de validación y otro para el de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoH2Nfgtk_-E"
      },
      "source": [
        "train_ds = Dataset(train_records, train_tfms)\n",
        "valid_ds = Dataset(valid_records, valid_tfms)\n",
        "test_ds = Dataset(test_records, valid_tfms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EchAR1rEJpZ"
      },
      "source": [
        "Una vez creados dichos datasets podemos mostrar imágenes de los mismos. En concreto la siguiente instrucción muestra imágenes del conjunto de entrenamiento a las cuáles se les han aplicado una serie de transformaciones. Es conveniente ejecutar esta visualización para comprobar que todo está correcto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uHT4HhElGgK"
      },
      "source": [
        "samples = [train_ds[0] for _ in range(3)]\n",
        "show_samples(samples, ncols=3, class_map=class_map, denormalize_fn=denormalize_imagenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vRablXEJpd"
      },
      "source": [
        "### DataLoaders\n",
        "\n",
        "El último paso es crear nuestros DataLoaders a partir de los datasets construidos anteriormente. Notar que cada modelo tiene su propio DataLoader. En este caso como vamos a crear un modelo de Faster RCNN debemos usar las siguientes instrucciones. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIplhRualIlE"
      },
      "source": [
        "train_dl = faster_rcnn.train_dl(train_ds, batch_size=8, num_workers=0, shuffle=True)\n",
        "valid_dl = faster_rcnn.valid_dl(valid_ds, batch_size=8, num_workers=0, shuffle=False)\n",
        "test_dl = faster_rcnn.valid_dl(test_ds, batch_size=8, num_workers=0, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWYmopYNEJqK"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Para crear y entrenar nuestro modelo debemos crear un objeto `Learner` de FastAI. Para crear dicho objeto, lo primero que debemos hacer es construir un modelo con la arquitectura que queremos usar, en este caso Faster RCNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhBJKvmKEJqL"
      },
      "source": [
        "model = faster_rcnn.model(num_classes=len(class_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlehdTsEJqM"
      },
      "source": [
        "A continuación debemos proporcionar las métricas que queremos utilizar para evaluar el modelo. Por el momento la única métrica soportada por IceVision es el mAP de COCO, por lo tanto utilizaremos dicha métrica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxSm9QU4lO_K"
      },
      "source": [
        "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPzfZN8tEJqO"
      },
      "source": [
        "Ya estamos listos para construir nuestro `Learner`. Notar que dicho objeto se construye de manera distinta dependiendo de la arquitectura que queramos utilizar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtuQ9Pj-lSoV"
      },
      "source": [
        "learn = faster_rcnn.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVc01_dDQntN"
      },
      "source": [
        "Ahora podemos entrenar nuestro modelo utilizando la técnica de fine tuning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVZJNDqJlZi5"
      },
      "source": [
        "learn.fine_tune(10,freeze_epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2chVdeuEJq5"
      },
      "source": [
        "Una vez finalizado el entrenamiento podemos guardar nuestro modelo del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYO1kWUDrngs"
      },
      "source": [
        "torch.save(model.state_dict(),'fasterRCNNFruits.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqhOHtotEJrB"
      },
      "source": [
        "### Evaluando el modelo\n",
        "\n",
        "Al igual que vimos para los modelos de clasificación, la métrica mostrada durante el proceso de entrenamiento se refiere al conjunto de validación, mientras que nos interesa saber el resultado obtenido para el conjunto de test. \n",
        "\n",
        "Para ello, lo primero que debemos hacer es construir un nuevo dataloader del siguiente modo, indicando que el conjunto de validación es el de test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrHxoeNRGAt"
      },
      "source": [
        "newdl = fastai.DataLoaders(faster_rcnn.fastai.convert_dataloader_to_fastai(train_dl),faster_rcnn.fastai.convert_dataloader_to_fastai(test_dl)).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sySQ0lUgEJrg"
      },
      "source": [
        "A continuación modificamos el dataloader del objeto `Learn` que hemos entrenado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1hYzealQYYC"
      },
      "source": [
        "learn.dls = newdl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQemIbWGEJro"
      },
      "source": [
        "Por último evaluamos nuestro modelo usando el método `validate()`. El método `validate()` devuelve dos valores, el valor de la pérdida y el valor de la métrica asociada al conjunto de validación, que en este caso es el de test. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms4LXdZ0LUV_"
      },
      "source": [
        "learn.validate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdUa7ocZEJrt"
      },
      "source": [
        "### Inferencia\n",
        "\n",
        "Vamos a ver cómo usar el modelo ante una nueva imagen. Para ello lo primero que vamos a hacer es cargar dicho modelo. Para ello debemos crear un modelo con la arquitectura que utilizamos (Faster RCNN en nuestro caso), y posteriormente cargar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eHIEmkfPgZq"
      },
      "source": [
        "model = faster_rcnn.model(num_classes=len(class_map))\n",
        "state_dict = torch.load('fasterRCNNFruits.pth')\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-j8R6KEJrv"
      },
      "source": [
        "El siguiente paso es cargar la imagen, para lo que usaremos la librería `PIL`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq044_u5MHHP"
      },
      "source": [
        "import PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TunuHhauMH_e"
      },
      "source": [
        "img = np.array(PIL.Image.open('fruits/test/images/mixed_25.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBUBoxNEJrx"
      },
      "source": [
        "La siguiente instrucción permite mostrar la imagen que acabamos de cargar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGrZkmcDMJXI"
      },
      "source": [
        "show_img(img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wypZ0ghDEJry"
      },
      "source": [
        "Ya estaríamos listos para relizar las predicciones sobre la imagen. Sin embargo, cabe recordar que primero debemos reescalar las imágenes y normalizarlas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g-twflKMFo9"
      },
      "source": [
        "infer_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size),tfms.A.Normalize()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axE1jQpIEJr0"
      },
      "source": [
        "El siguiente paso es crear un dataset (podemos cargar varias imágenes en el mismo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWMTK4xoMKnG"
      },
      "source": [
        "infer_ds = Dataset.from_images([img], infer_tfms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwlLbikUEJr9"
      },
      "source": [
        "Y por último realizamos las predicciones llamando a `build_infer_batch` y a continuación a `predict` (notar que estos métodos son dependientes de la arquirectura que hayamos utilizado). Notar también que en el método `predict` podemos fijar el nivel de confianza mínimo para realizar la predicción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nEvXp7dMMZI"
      },
      "source": [
        "batch, samples = faster_rcnn.build_infer_batch(infer_ds)\n",
        "preds = faster_rcnn.predict(model=model, batch=batch,detection_threshold=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQJ_iEVsEJsA"
      },
      "source": [
        "Por último podemos mostrar las predicciones que se han realizado del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd6j6v0sMN3O"
      },
      "source": [
        "imgs = [sample[\"img\"] for sample in samples]\n",
        "show_preds(\n",
        "    samples=imgs,\n",
        "    preds=preds,\n",
        "    class_map=class_map,\n",
        "    denormalize_fn=denormalize_imagenet,\n",
        "    show=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECCbhDeEJsD"
      },
      "source": [
        "## Ejercicio \n",
        "\n",
        "A continuación se listan una serie de datasets que están disponibles en Kaggle o en otras fuentes. El ejercicio que tenéis que realizar consiste en elegir uno de esos datasets (también podéis elegir otro que vosotros prefiráis) y construir un modelo siguiendo las instrucciones proporcionadas a lo largo de este notebook. \n",
        "\n",
        "- [Kangaroo dataset](https://github.com/experiencor/kangaroo).\n",
        "- [Racoon dataset](https://github.com/datitran/raccoon_dataset).\n",
        "- [Wheat detection](https://www.kaggle.com/c/global-wheat-detection/data).\n",
        "- [Winegrape Detection Dataset](https://github.com/thsant/wgisd). \n",
        "\n",
        "También puedes elegir cualquiera de los datasets disponibles en [50+ Object Detection Datasets from different industry domains](https://towardsai.net/p/computer-vision/50-object-detection-datasets-from-different-industry-domains) o utilizar un dataset de otra fuente. **Ojo.** Cuidado con el tamaño de los datasets, si contienen muchas imágenes el proceso de entrenamiento puede ser muy largo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw3FHBNjEJsF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}