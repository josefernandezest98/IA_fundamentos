{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practica9_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IA2021UR/practica-9-joferne/blob/main/practica9_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXROQV9uZ2n"
      },
      "source": [
        "# Práctica 9 Parte 3: Desarrollando un modelo de lenguaje para generar texto\n",
        "\n",
        "Un modelo de lenguaje puede predecir la siguiente palabra de una secuencia basándose en palabras observadas anteriormente. Las redes neuronales son el método más utilizado para desarrollar este tipo de modelos porque pueden usar una representación donde palabras con significados similares tienen representaciones similares. \n",
        "\n",
        "En esta parte de la práctica vamos a ver cómo generar uno de esos modelos. \n",
        "\n",
        "Este notebook está basado en el libro Deep Learning for Natural Language Processing de Jason Brownlee. \n",
        "\n",
        "Es importante que tengas activado el uso de **GPU** en el notebook de colab (menú Edit -> Notebook Settings -> Hardware accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fHn2VKwO0E"
      },
      "source": [
        "## La República de Platón\n",
        "\n",
        "Nuestro modelo de lenguaje va a estar basado en la república de Platón. Este libro está estructurado en forma de una conversación que trata el tema del orden y la justicia dentro de una ciudad. El texto completo está disponible para el dominio público dentro del [proyecto Gutenberg](http://www.gutenberg.org/).\n",
        "\n",
        "Este libro de Platón está disponible en varios formatos en el [proyecto Gutenberg](http://www.gutenberg.org/cache/epub/1497/pg1497.txt). La versión que nos interesa a nosotros es la versión ASCII del libro. Con la siguiente instrucción puedes descargar el libro donde se han eliminado la portada y la contraportada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmhqeiuuFYV",
        "outputId": "ba69adc9-5e6e-4f64-b086-11e7166da699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt -O republic.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-13 15:22:30--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657826 (642K) [text/plain]\n",
            "Saving to: ‘republic.txt’\n",
            "\n",
            "\rrepublic.txt          0%[                    ]       0  --.-KB/s               \rrepublic.txt        100%[===================>] 642.41K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-05-13 15:22:30 (63.7 MB/s) - ‘republic.txt’ saved [657826/657826]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_hckdhxWdz"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Vamos a preparar los datos para construir nuestro modelo. \n",
        "\n",
        "### Revisando el texto\n",
        "\n",
        "Vamos a comenzar revisando parte del texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn3g3XJYxSTO",
        "outputId": "51b274fb-35b3-4b0d-f560-f82c88245009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -30 republic.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I.\r\n",
            "\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\n",
            "distance as we were starting on our way home, and told his servant to\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\n",
            "\r\n",
            "I turned round, and asked him where his master was.\r\n",
            "\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\n",
            "\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\n",
            "of Nicias, and several others who had been at the procession.\r\n",
            "\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\n",
            "companion are already on your way to the city.\r\n",
            "\r\n",
            "You are not far wrong, I said.\r\n",
            "\r\n",
            "But do you see, he rejoined, how many we are?\r\n",
            "\r\n",
            "Of course.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4uFv8RWzVn-"
      },
      "source": [
        "A partir de un rápido vistazo al fragmento de texto anterior podemos ver ciertas cuestiones que tendremos que procesar:\n",
        "- Las cabeceras de los capítulos.\n",
        "- Muchos signos de puntuación.\n",
        "- Nombres extraños.\n",
        "- Algunos monólogos muy largos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfXxOz0GQT"
      },
      "source": [
        "### Cargando el texto\n",
        "\n",
        "El primer paso consiste en cargar el texto en memoria. Podemos desarrollar una pequeña función que se encargue de esto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dDTRFi4xmBr"
      },
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdKtPFM0aZ-"
      },
      "source": [
        "Usando dicha función podemos cargar nuestro fichero del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_oUjl_Q0Zgn"
      },
      "source": [
        "in_filename = 'republic.txt'\n",
        "doc = load_doc(in_filename)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfi8WHI0kKb"
      },
      "source": [
        "Ahora podemos mostrar parte de dicho texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZRb4q6W0jQT",
        "outputId": "90197724-fcf7-411d-e6d3-da7ace358d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(doc[:200])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4PZ32G0reo"
      },
      "source": [
        "### Limpiando el texto\n",
        "\n",
        "Ahora necesitamos transformar el texto en bruto a una secuencia de tokens (o palabras) que podamos usar para entrenar nuestro modelo. \n",
        "\n",
        "Vamos a aplicar las siguientes operaciones para limpiar nuestro texto:\n",
        "- Reemplazar todas las ocurrencias de '-' con un espacio en blanco de manera que podamos partir mejor las palabras.\n",
        "- Partir las palabras basándonos en espacios en blanco.\n",
        "- Eliminar todos los símbolos de puntuación.\n",
        "- Eliminar todas las palabras que no son alfabéticas. \n",
        "- Normalizar todas las palabras a minúsculas.\n",
        "\n",
        "La mayoría de estas transformaciones tienen como objetivo reducir el tamaño del vocabulario. Un tamaño de vocabulario excesivamente grande es un problema cuando se intenta crear modelos de lenguaje. Vocabularios pequeños producen modelos más pequeños que se entrenan más rápidos.\n",
        "\n",
        "Vamos a implementar todas las operaciones de limpieza en la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsZiyRsE0niT"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVV2RsWB2kOD"
      },
      "source": [
        "Procedemos a limpiar nuestro documento y a continuación mostramos algunas estadísticas sobre nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUUCtGKP2jdA",
        "outputId": "74d5e14c-9ca2-4421-ca93-065ad4921db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOBzRk232uAv",
        "outputId": "26922233-63ac-44a9-d991-2cf4f33eb5db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvkoWbVh25cR"
      },
      "source": [
        "Es decir, nuestro modelo consta de una 7500 palabras. Este tamaño de vocabulario es pequeño y va a ser manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlhVwpz3CQi"
      },
      "source": [
        "### Guardando el texto limpio\n",
        "\n",
        "Vamos a organizar la larga lista de tokens en secuencias de 50 palabras de entrada y 1 palabra de salida (esto servirá para luego entrenar nuestro modelo). \n",
        "\n",
        "Este proceso lo implementamos con la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtEe2J7E21am"
      },
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6upZchl37wM"
      },
      "source": [
        "Organizamos nuestros tokens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpcwuFBp35XJ"
      },
      "source": [
        "lines = organize_tokens(tokens)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyDfPjA4Bfr"
      },
      "source": [
        "Ahora vamos a guardar las secuencias en un nuevo fichero para poder cargarlo en el futuro. Para ello nos definimos la siguiente función que guardará cada elemento de la secuencia en una línea del fichero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxPoCvn4AfS"
      },
      "source": [
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdjRviE4Wfd"
      },
      "source": [
        "Podemos llamar a la función anterior para guardar nuestro fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbncNaRo4T74"
      },
      "source": [
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGMZ3yW4m9O"
      },
      "source": [
        "Podemos ver parte de dicho fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hitWeMb84e1R",
        "outputId": "38f4fd13-f620-4f79-824f-fdc006cff41d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -5 republic_sequences.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted\n",
            "i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with\n",
            "went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the\n",
            "down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Do4ht04u3Q"
      },
      "source": [
        "## Entrenando el modelo de lenguaje\n",
        "\n",
        "Vamosa  entrenar ahora nuestro modelo a partir de los datos que hemos preparado. Dicho modelo tendrá ciertas características:\n",
        "- Usará una representación para las palabras de manera que palabras diferentes con significados similares tendrán una representación similar.\n",
        "- La representación será aprendida al mismo tiempo que se aprende el modelo.\n",
        "- Aprenderá a predecir la probabilidad de la siguiente palabra a partir del contexto de las últimas 100 palabras.\n",
        "\n",
        "En concreto para implementar este modelo vamos a usar una capa de Embedding para aprender la representación de las palabras, y una red neuronal recurrente con capas LSTM para predecir nuevas palabras basándonos en el contexto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_vMHL65cx2"
      },
      "source": [
        "### Cargando las secuencias\n",
        "\n",
        "Podemos comenzar cargando las secuencias que hemos guardado anteriormente. En este caso este paso no sería necesario ya que el proceso de generación de las secuencias es bastante rápido, pero si estamos trabajando con un dataset más grande sí que puede ser conveniente. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mChmy3q04rJq"
      },
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au4s6vM7UED"
      },
      "source": [
        "### Codificando las secuencias\n",
        "\n",
        "Las capas de Embedding esperan que las secuencias de entrada estén compuestas de vectores de enteros. Para ello vamos a identificar cada palabra de nuestro vocabulario con un entero único y codificarlo en una secuencia de entrada. En el futuro cuando vayamos a realizar las predicciones tendremos que realizar el proceso inverso.\n",
        "\n",
        "Para llevar a cabo este proceso de tokenización vamos a usar la API de Keras del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVH8p7Cv52Zo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_odT9JZ710u"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEQoF2S478z9"
      },
      "source": [
        "Ahora podemos acceder a los identificadores de cada palabra usando el atributo ``word_index`` del objeto ``Tokenizer`` que hemos creado. \n",
        "\n",
        "Además debemos determinar el tamaño de nuestro vocabulario para definir la capa de embedding. En concreto, a las palabras de nuestro vocabulario se les han asignado valores entre 1 y el número total de palabras de nuestro vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byu4Vfe075Aj"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaF1yDTF9elO"
      },
      "source": [
        "### Secuencias de entrada y salida\n",
        "\n",
        "Una vez que tenemos codificadas nuestras secuencias tenemos que separarlas en elementos de entrada ($X$) y de salida ($y$). Después de realizar la separación debemos codificar cada palabra usando el método one-hot. Este proceso lo llevaremos a cabo mediante la función ``to_categorical()`` de Keras.\n",
        "Finalmente necesitamos especificar cómo de largas serán las secuencias de entrada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SekaGTwl9eCl"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUMEkkh-gSm"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Ahora podemos definir nuestro modelo que constará de una capa de Embedding, seguida de dos capas LSTM y terminando con una red completamente conectada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs_7VIsS-ODn"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMo-mV9E_cco"
      },
      "source": [
        "Pasamos a entrenar nuestro modelo. Como este proceso es bastante costoso (incluso usando GPUs) en la siguiente sección se proporcionan los ficheros necesarios para usar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxHVuAuCdzC",
        "outputId": "882c861b-d8bf-4b48-af13-28c205c1b845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d497f858600f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EeUlPGxEifh"
      },
      "source": [
        "Una vez entrenado podemos guardar los pesos del modelo y el tokenizador. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKc5WObR_maO",
        "outputId": "b562cd1d-3d86-4d59-dc89-b132a82c558d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "model.save_weights('./model.h5', overwrite=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b540a1e4ce8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzX5deoEuiw",
        "outputId": "dc337ad2-5124-490b-bf43-3434513995a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "from pickle import dump\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0e8eddf94fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnE1BOhEuIz"
      },
      "source": [
        "## Usando el modelo\n",
        "\n",
        "Como has podido ver en el paso anterior, el proceso de entrenar este tipo de modelos es muy costoso, por lo que puedes descargar los ficheros necesarios para usar el modelo desde el siguiente enlace. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJozw5qBFUoO",
        "outputId": "6fe12888-9383-444b-deee-907ef22fcad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5 -O model.h5\n",
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl -O tokenizer.pkl"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-13 15:23:20--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101248 (4.9M) [application/octet-stream]\n",
            "Saving to: ‘model.h5’\n",
            "\n",
            "model.h5            100%[===================>]   4.86M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-05-13 15:23:21 (74.5 MB/s) - ‘model.h5’ saved [5101248/5101248]\n",
            "\n",
            "--2021-05-13 15:23:21--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353379 (345K) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.pkl’\n",
            "\n",
            "tokenizer.pkl       100%[===================>] 345.10K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2021-05-13 15:23:21 (44.2 MB/s) - ‘tokenizer.pkl’ saved [353379/353379]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbxpdyp4FZ0t"
      },
      "source": [
        "### Cargando los datos\n",
        "\n",
        "Comenzamos cargando nuestros datos al igual que antes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwREuZMiFfVT"
      },
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIu2LzlFl5v"
      },
      "source": [
        "Necesitamos este texto para elegir una secuencia de inicio que será la entrada para nuestro modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-IslrBxFlnA"
      },
      "source": [
        "seq_length = len(lines[0].split())-1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCQcIN_Fxr2"
      },
      "source": [
        "### Cargando el modelo\n",
        "\n",
        "Vamos a cargar el modelo y a fijar los pesos. Notar que para este paso ya no necesitamos el uso de TPU, y que el modelo podría ser usado en cualquier ordenador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usmQZBvYF7Q1"
      },
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvlOps3GG05"
      },
      "source": [
        "También necesitamos cargar el tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xib_xurDGIpL"
      },
      "source": [
        "from pickle import load\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrPda8NGUv7"
      },
      "source": [
        "### Generando texto\n",
        "\n",
        "El primer paso para generar el texto consiste en preparar una entrada, para lo cual elegiremos una línea aleatoria del texto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EDkPYM5Gdmz",
        "outputId": "5e7d4c83-d0a7-4fe0-e2f2-ae0676392a7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a rough and ready cure an emetic or a purge or a cautery or the knife these are his remedies and if some one prescribes for him a course of dietetics and tells him that he must swathe and swaddle his head and all that sort of thing he replies at\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZO-Teo5Go9O"
      },
      "source": [
        "A continuación podemos generar nuevas palabras una por una. Primero, el texto debe codificarse usando el tokenizer que hemos cargado anteriormente. Ahora el modelo puede predecir nuevas palabras usando el método ``predict_classes()`` que devuelve el índice de la palabra con probabilidad más alta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpLQAtHMEY"
      },
      "source": [
        "Esta palabra se añade a nuestro texto inicial y se repite el proceso. Notar que esta secuencia va a ir creciendo por lo que tendremos que truncarla, para lo que utilizamos la función ``pad_sequences()`` de Keras. Todo este proceso se puede implementar con la siguiente función. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4J29cLaHZx-"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict_classes(encoded,verbose=0)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6dQKHIIIot"
      },
      "source": [
        "Ahora podemos generar una nueva secuencia usando el siguiente código. Cada vez que lo ejecutemos obtendremos un resultado distinto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUn97EmILPG",
        "outputId": "dcd80bb3-a3f9-4b5d-941e-92a92c517b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "soul on which they mightily fasten imparting grace and making the soul of him who is rightly educated graceful or of him who is illeducated ungraceful and also because he who has received this true education of the inner being will most shrewdly perceive omissions or faults in art and nature\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "and the other excellences of peirithous taking who have won a man who is wealthy such magic degraded and do you mean that the other features their mode of speaking and the other principle is arrayed on the state and the other timid the followers and the other plunged in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlmSJGumIepK"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Elige tu propio libro del proyecto Gutenberg (es posible usar libros en [español](https://www.gutenberg.org/browse/languages/es)) y crea tu propio modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffe564PUWszX"
      },
      "source": [
        "Descargamos el libro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jsZvvYI21o",
        "outputId": "c15f0c01-d444-4a11-a4d2-fd5fcb3bb4c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://www.gutenberg.org/cache/epub/1/pg1.txt.utf8 -O texto.txt"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-13 15:30:57--  https://www.gutenberg.org/cache/epub/1/pg1.txt.utf8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120946 (118K) [text/plain]\n",
            "Saving to: ‘texto.txt’\n",
            "\n",
            "texto.txt           100%[===================>] 118.11K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-05-13 15:30:58 (3.48 MB/s) - ‘texto.txt’ saved [120946/120946]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXpdwGabW8yh"
      },
      "source": [
        "Observamos el principio del texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHuos7ELViSa",
        "outputId": "892b89bc-79de-4f2d-efad-a4488bd98cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -30 texto.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\r\n",
            "\r\n",
            "===========================================================\r\n",
            "\r\n",
            "     NOTE:  This file combines the first two Project Gutenberg\r\n",
            "     files, both of which were given the filenumber #1. There are\r\n",
            "     several duplicate files here. There were many updates over\r\n",
            "     the years.  All of the original files are included in the\r\n",
            "     \"old\" subdirectory which may be accessed under the \"More\r\n",
            "     Files\" listing in the PG Catalog of this file. No changes\r\n",
            "     have been made in these original etexts.\r\n",
            "\r\n",
            "===========================================================\r\n",
            "\r\n",
            "\r\n",
            "**Welcome To The World of Free Plain Vanilla Electronic Texts**\r\n",
            "\r\n",
            "**Etexts Readable By Both Humans and By Computers, Since 1971**\r\n",
            "\r\n",
            "*These Etexts Prepared By Hundreds of Volunteers and Donations*\r\n",
            "\r\n",
            "Below you will find the first nine Project Gutenberg Etexts, in\r\n",
            "one file, with one header for the entire file.  This is to keep\r\n",
            "the overhead down, and in response to requests from Gopher site\r\n",
            "keeper to eliminate as much of the headers as possible.\r\n",
            "\r\n",
            "However, for legal and financial reasons, we must request these\r\n",
            "headers be left at the beginning of each file that is posted in\r\n",
            "any general user areas, as Project Gutenberg is run mostly by a\r\n",
            "donation from people like you.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns63rqSHXR9V"
      },
      "source": [
        "Cargamos el texto en memoria y mostramos parte del fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfrYQtukViPR",
        "outputId": "0499e425-b129-4fe0-bd3e-f9357c9cafa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def load_doc(filename):\n",
        "  # Abrimos el fichero en modo lectura\n",
        "  file = open(filename,'r')\n",
        "  # Leemos el texto completo\n",
        "  text = file.read()\n",
        "  # Cerramos el fichero\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "in_filename = 'texto.txt'\n",
        "doc = load_doc(in_filename)\n",
        "\n",
        "print(doc[:200])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "\n",
            "===========================================================\n",
            "\n",
            "     NOTE:  This file combines the first two Project Gutenberg\n",
            "     files, both of which were given the filenumber #1. There are\n",
            "     se\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUi0ED3BXuJH"
      },
      "source": [
        "Limpiamos el documento y mostramos algunas estadísticas sobre el texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kyVFEbyViMp",
        "outputId": "676a7447-0a9e-4060-a4f6-acb41d48997c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "  # Reemplazar '--' con un espacio en blanco ' '\n",
        "  doc = doc.replace('--',' ')\n",
        "  # Partir palabras basándonos en espacios en blanco\n",
        "  tokens = doc.split()\n",
        "  # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  # Eliminamos los símbolos de puntuación\n",
        "  tokens = [re_punc.sub('',w) for w in tokens]\n",
        "  # Eliminamos elementos que nos son alfabéticos\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # Convertimos a minúsculas\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['note', 'this', 'file', 'combines', 'the', 'first', 'two', 'project', 'gutenberg', 'files', 'both', 'of', 'which', 'were', 'given', 'the', 'filenumber', 'there', 'are', 'several', 'duplicate', 'files', 'here', 'there', 'were', 'many', 'updates', 'over', 'the', 'years', 'all', 'of', 'the', 'original', 'files', 'are', 'included', 'in', 'the', 'old', 'subdirectory', 'which', 'may', 'be', 'accessed', 'under', 'the', 'more', 'files', 'listing', 'in', 'the', 'pg', 'catalog', 'of', 'this', 'file', 'no', 'changes', 'have', 'been', 'made', 'in', 'these', 'original', 'etexts', 'welcome', 'to', 'the', 'world', 'of', 'free', 'plain', 'vanilla', 'electronic', 'texts', 'etexts', 'readable', 'by', 'both', 'humans', 'and', 'by', 'computers', 'since', 'these', 'etexts', 'prepared', 'by', 'hundreds', 'of', 'volunteers', 'and', 'donations', 'below', 'you', 'will', 'find', 'the', 'first', 'nine', 'project', 'gutenberg', 'etexts', 'in', 'one', 'file', 'with', 'one', 'header', 'for', 'the', 'entire', 'file', 'this', 'is', 'to', 'keep', 'the', 'overhead', 'down', 'and', 'in', 'response', 'to', 'requests', 'from', 'gopher', 'site', 'keeper', 'to', 'eliminate', 'as', 'much', 'of', 'the', 'headers', 'as', 'possible', 'however', 'for', 'legal', 'and', 'financial', 'reasons', 'we', 'must', 'request', 'these', 'headers', 'be', 'left', 'at', 'the', 'beginning', 'of', 'each', 'file', 'that', 'is', 'posted', 'in', 'any', 'general', 'user', 'areas', 'as', 'project', 'gutenberg', 'is', 'run', 'mostly', 'by', 'a', 'donation', 'from', 'people', 'like', 'you', 'if', 'you', 'see', 'our', 'books', 'posted', 'anywhere', 'without', 'these', 'headers', 'you', 'are', 'requested', 'to', 'send', 'them', 'a', 'note', 'requesting', 'they', 'reattach']\n",
            "Total Tokens: 19582\n",
            "Unique Tokens: 3151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl6OKXqaYLh_"
      },
      "source": [
        "Guardamos el texto limpio y mostramos cómo quedaría."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjp_kpfTViHU",
        "outputId": "2cbd147d-4956-4955-c7d2-d91d90c120dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "  length = input_len + output_len\n",
        "  sequences = list()\n",
        "  for i in range(length,len(tokens)):\n",
        "    # Elegimos la secuencia de tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # Convertimos la secuencia en una línea\n",
        "    line = ' '.join(seq)\n",
        "    # Almacenamos el resultado\n",
        "    sequences.append(line)\n",
        "  return sequences\n",
        "\n",
        "lines = organize_tokens(tokens)\n",
        "\n",
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "out_filename = 'texto_limpio.txt'\n",
        "save_doc(lines,out_filename)\n",
        "\n",
        "!head -5 texto_limpio.txt"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "note this file combines the first two project gutenberg files both of which were given the filenumber there are several duplicate files here there were many updates over the years all of the original files are included in the old subdirectory which may be accessed under the more files listing in\n",
            "this file combines the first two project gutenberg files both of which were given the filenumber there are several duplicate files here there were many updates over the years all of the original files are included in the old subdirectory which may be accessed under the more files listing in the\n",
            "file combines the first two project gutenberg files both of which were given the filenumber there are several duplicate files here there were many updates over the years all of the original files are included in the old subdirectory which may be accessed under the more files listing in the pg\n",
            "combines the first two project gutenberg files both of which were given the filenumber there are several duplicate files here there were many updates over the years all of the original files are included in the old subdirectory which may be accessed under the more files listing in the pg catalog\n",
            "the first two project gutenberg files both of which were given the filenumber there are several duplicate files here there were many updates over the years all of the original files are included in the old subdirectory which may be accessed under the more files listing in the pg catalog of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWWgOWf1Yp6d"
      },
      "source": [
        "Cargamos y codificamos las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvJj3msjVh8C"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "in_filename = 'texto_limpio.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1 "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaFpmTNNY_5L"
      },
      "source": [
        "Separamos secuencias en entrada y salida y entrenamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3F7iLu_Vh0L",
        "outputId": "7f7550f1-0c39-4709-f9e2-ad8cd0fb4208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "def define_model(vocab_size,seq_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "  model.add(LSTM(100,return_sequences=True))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(vocab_size,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "153/153 [==============================] - 6s 22ms/step - loss: 7.0969 - accuracy: 0.0559\n",
            "Epoch 2/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 6.2558 - accuracy: 0.0706\n",
            "Epoch 3/100\n",
            "153/153 [==============================] - 5s 30ms/step - loss: 6.1505 - accuracy: 0.0669\n",
            "Epoch 4/100\n",
            "153/153 [==============================] - 4s 27ms/step - loss: 5.9701 - accuracy: 0.0904\n",
            "Epoch 5/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 5.7854 - accuracy: 0.0982\n",
            "Epoch 6/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 5.6859 - accuracy: 0.1013\n",
            "Epoch 7/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 5.6056 - accuracy: 0.1093\n",
            "Epoch 8/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 5.4933 - accuracy: 0.1141\n",
            "Epoch 9/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 5.3856 - accuracy: 0.1188\n",
            "Epoch 10/100\n",
            "153/153 [==============================] - 4s 25ms/step - loss: 5.3042 - accuracy: 0.1216\n",
            "Epoch 11/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 5.2010 - accuracy: 0.1280\n",
            "Epoch 12/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 5.0876 - accuracy: 0.1307\n",
            "Epoch 13/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 5.0523 - accuracy: 0.1295\n",
            "Epoch 14/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 4.9398 - accuracy: 0.1395\n",
            "Epoch 15/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 4.8490 - accuracy: 0.1503\n",
            "Epoch 16/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 4.7330 - accuracy: 0.1585\n",
            "Epoch 17/100\n",
            "153/153 [==============================] - 4s 26ms/step - loss: 4.6544 - accuracy: 0.1662\n",
            "Epoch 18/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 4.5604 - accuracy: 0.1671\n",
            "Epoch 19/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 4.4398 - accuracy: 0.1796\n",
            "Epoch 20/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 4.3658 - accuracy: 0.1826\n",
            "Epoch 21/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 4.2452 - accuracy: 0.1937\n",
            "Epoch 22/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 4.1981 - accuracy: 0.1870\n",
            "Epoch 23/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 4.0768 - accuracy: 0.2015\n",
            "Epoch 24/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 3.9974 - accuracy: 0.2041\n",
            "Epoch 25/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 3.9109 - accuracy: 0.2094\n",
            "Epoch 26/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 3.8610 - accuracy: 0.2143\n",
            "Epoch 27/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 3.7583 - accuracy: 0.2235\n",
            "Epoch 28/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 3.6797 - accuracy: 0.2358\n",
            "Epoch 29/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 3.5719 - accuracy: 0.2430\n",
            "Epoch 30/100\n",
            "153/153 [==============================] - 4s 25ms/step - loss: 3.5232 - accuracy: 0.2495\n",
            "Epoch 31/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 3.4282 - accuracy: 0.2702\n",
            "Epoch 32/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 3.3676 - accuracy: 0.2723\n",
            "Epoch 33/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 3.3220 - accuracy: 0.2742\n",
            "Epoch 34/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 3.2286 - accuracy: 0.2923\n",
            "Epoch 35/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 3.1690 - accuracy: 0.2993\n",
            "Epoch 36/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 3.0956 - accuracy: 0.3179\n",
            "Epoch 37/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 3.0345 - accuracy: 0.3235\n",
            "Epoch 38/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 2.9796 - accuracy: 0.3330\n",
            "Epoch 39/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 2.9011 - accuracy: 0.3448\n",
            "Epoch 40/100\n",
            "153/153 [==============================] - 3s 18ms/step - loss: 2.8576 - accuracy: 0.3510\n",
            "Epoch 41/100\n",
            "153/153 [==============================] - 3s 18ms/step - loss: 2.7996 - accuracy: 0.3644\n",
            "Epoch 42/100\n",
            "153/153 [==============================] - 3s 17ms/step - loss: 2.7497 - accuracy: 0.3701\n",
            "Epoch 43/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.6802 - accuracy: 0.3845\n",
            "Epoch 44/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.6319 - accuracy: 0.3891\n",
            "Epoch 45/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 2.5813 - accuracy: 0.4011\n",
            "Epoch 46/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.5516 - accuracy: 0.4112\n",
            "Epoch 47/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 2.4816 - accuracy: 0.4231\n",
            "Epoch 48/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 2.4610 - accuracy: 0.4290\n",
            "Epoch 49/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 2.4228 - accuracy: 0.4366\n",
            "Epoch 50/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 2.3707 - accuracy: 0.4423\n",
            "Epoch 51/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 2.3138 - accuracy: 0.4531\n",
            "Epoch 52/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 2.2762 - accuracy: 0.4615\n",
            "Epoch 53/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 2.2573 - accuracy: 0.4705\n",
            "Epoch 54/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 2.1951 - accuracy: 0.4815\n",
            "Epoch 55/100\n",
            "153/153 [==============================] - 4s 27ms/step - loss: 2.1718 - accuracy: 0.4842\n",
            "Epoch 56/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.1674 - accuracy: 0.4871\n",
            "Epoch 57/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.1057 - accuracy: 0.4989\n",
            "Epoch 58/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.0883 - accuracy: 0.5013\n",
            "Epoch 59/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 2.0439 - accuracy: 0.5061\n",
            "Epoch 60/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 1.9945 - accuracy: 0.5224\n",
            "Epoch 61/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 1.9581 - accuracy: 0.5256\n",
            "Epoch 62/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 1.9388 - accuracy: 0.5334\n",
            "Epoch 63/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 1.9357 - accuracy: 0.5330\n",
            "Epoch 64/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 1.8904 - accuracy: 0.5438\n",
            "Epoch 65/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 1.8422 - accuracy: 0.5511\n",
            "Epoch 66/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 1.8186 - accuracy: 0.5599\n",
            "Epoch 67/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 1.7894 - accuracy: 0.5629\n",
            "Epoch 68/100\n",
            "153/153 [==============================] - 4s 28ms/step - loss: 1.7741 - accuracy: 0.5676\n",
            "Epoch 69/100\n",
            "153/153 [==============================] - 4s 28ms/step - loss: 1.7601 - accuracy: 0.5686\n",
            "Epoch 70/100\n",
            "153/153 [==============================] - 4s 27ms/step - loss: 1.7250 - accuracy: 0.5769\n",
            "Epoch 71/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 1.7232 - accuracy: 0.5804\n",
            "Epoch 72/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 1.6917 - accuracy: 0.5864\n",
            "Epoch 73/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 1.6818 - accuracy: 0.5864\n",
            "Epoch 74/100\n",
            "153/153 [==============================] - 4s 25ms/step - loss: 1.6472 - accuracy: 0.5929\n",
            "Epoch 75/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 1.6013 - accuracy: 0.6041\n",
            "Epoch 76/100\n",
            "153/153 [==============================] - 4s 28ms/step - loss: 1.5964 - accuracy: 0.6033\n",
            "Epoch 77/100\n",
            "153/153 [==============================] - 3s 23ms/step - loss: 1.5912 - accuracy: 0.6090\n",
            "Epoch 78/100\n",
            "153/153 [==============================] - 4s 26ms/step - loss: 1.5711 - accuracy: 0.6114\n",
            "Epoch 79/100\n",
            "153/153 [==============================] - 4s 26ms/step - loss: 1.5378 - accuracy: 0.6243\n",
            "Epoch 80/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 1.5013 - accuracy: 0.6274\n",
            "Epoch 81/100\n",
            "153/153 [==============================] - 4s 28ms/step - loss: 1.4983 - accuracy: 0.6233\n",
            "Epoch 82/100\n",
            "153/153 [==============================] - 4s 25ms/step - loss: 1.4723 - accuracy: 0.6346\n",
            "Epoch 83/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 1.4609 - accuracy: 0.6346\n",
            "Epoch 84/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 1.4408 - accuracy: 0.6395\n",
            "Epoch 85/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 1.4139 - accuracy: 0.6497\n",
            "Epoch 86/100\n",
            "153/153 [==============================] - 3s 22ms/step - loss: 1.4043 - accuracy: 0.6495\n",
            "Epoch 87/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 1.3784 - accuracy: 0.6568\n",
            "Epoch 88/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 1.3716 - accuracy: 0.6582\n",
            "Epoch 89/100\n",
            "153/153 [==============================] - 4s 28ms/step - loss: 1.3849 - accuracy: 0.6506\n",
            "Epoch 90/100\n",
            "153/153 [==============================] - 5s 30ms/step - loss: 1.3245 - accuracy: 0.6648\n",
            "Epoch 91/100\n",
            "153/153 [==============================] - 4s 27ms/step - loss: 1.2933 - accuracy: 0.6775\n",
            "Epoch 92/100\n",
            "153/153 [==============================] - 4s 23ms/step - loss: 1.2913 - accuracy: 0.6781\n",
            "Epoch 93/100\n",
            "153/153 [==============================] - 4s 27ms/step - loss: 1.2775 - accuracy: 0.6795\n",
            "Epoch 94/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 1.2702 - accuracy: 0.6826\n",
            "Epoch 95/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 1.2459 - accuracy: 0.6882\n",
            "Epoch 96/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 1.2281 - accuracy: 0.6880\n",
            "Epoch 97/100\n",
            "153/153 [==============================] - 3s 19ms/step - loss: 1.1949 - accuracy: 0.6980\n",
            "Epoch 98/100\n",
            "153/153 [==============================] - 3s 20ms/step - loss: 1.2239 - accuracy: 0.6936\n",
            "Epoch 99/100\n",
            "153/153 [==============================] - 4s 24ms/step - loss: 1.1738 - accuracy: 0.7123\n",
            "Epoch 100/100\n",
            "153/153 [==============================] - 3s 21ms/step - loss: 1.1556 - accuracy: 0.7129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63fd0281d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qXpGidZRNd"
      },
      "source": [
        "Guardamos los pesos del modelo y el Tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9VAFcXvVhuY"
      },
      "source": [
        "from pickle import dump\n",
        "model.save_weights('./model.h5', overwrite=True)\n",
        "\n",
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePzd_pmUabhz"
      },
      "source": [
        "Cargamos los datos y el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyJ0zn2VVhq6"
      },
      "source": [
        "from pickle import load\n",
        "\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "seq_length = len(lines[0].split())-1\n",
        "\n",
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')\n",
        "\n",
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY0R_Ho-a0CT"
      },
      "source": [
        "Generamos el texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m_ekzA0Vhns",
        "outputId": "e7b8fe4b-59c8-4935-b489-23120997a4bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import randint\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "    yhat = model.predict_classes(encoded,verbose=0)\n",
        "    out_word = ''\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rightly answer that question whichever of the two are best able to guard the laws and institutions of our state let them be our guardians very good neither i said can there be any question that the guardian who is to keep anything should have eyes rather than no eyes there\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071rLTu6a5-R"
      },
      "source": [
        "Generamos una nueva secuencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcGTwU5ZVhkf",
        "outputId": "b059fbbf-f9e8-4db0-b140-e79beee47dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress certainly not and again if he is forgetful and retains nothing of what he learns will he not be an empty vessel that is certain labouring in vain he must end in hating himself and his fruitless occupation yes then a soul which forgets cannot be ranked among genuine philosophic\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the accused shall enjoy virtue shall be used to be a witness and if we how it must needs be are attempted we need these just god about great donations donations if you wish to be pursued revived without man would independent among himself for every country a grand and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG188-6TI3ph"
      },
      "source": [
        "Recuerda guardar este notebook en tu repositorio usando la opción \"Save in GitHub\" del menú File."
      ]
    }
  ]
}