{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "instrucciones.ipynb",
      "provenance": []
    },
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10en6BLRzXL3"
      },
      "source": [
        "# Entregable 3: Un modelo para la clasificación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKRn-OllzXL8"
      },
      "source": [
        "En este documento vamos a ver cómo podemos utilizar la técnica de transfer-learning para crear un modelo de clasificación de texto, en concreto crearemos un modelo para decir si una valoración de IMDb es positiva o negativa. El contenido de este notebook está basado en gran medida en el libro de [FastAI](https://github.com/fastai/fastbook) y se usa un dataset en inglés, pero las mismas ideas se podrían aplicar para crear modelos en castellano o en otros lenguajes. \n",
        "\n",
        "**Importante:** Recuerda tener activado el uso de GPU para el notebook, de lo contrario no podrás entrenar los modelos. \n",
        "\n",
        "Para aplicar la técnica de transfer-learning será necesario usar un modelo de lenguaje. Como hemos visto en teoría, un modelo de lenguaje es un modelo encargado de predecir la siguiente palabra de un texto a partir de las que han aparecido atnes. Este tipo de tareas se conoce como *aprendizaje semi-supervisado*: no es necesario proporcionar etiquetas a nuestro modelo, sino simplemente le tenemos que proporcionar grandes cantidades de texto. Este proceso obtiene de manera automática las etiquetas a partir de los datos, y es una tarea que está lejos de ser trivial: advinar la siguiente palabra de un texto requiere una comprensión del lenguaje. Las técnicas de aprendizaje semi-supervisado también pueden ser usadas en otros contextos como la [visión por computador](https://www.fast.ai/2020/01/13/self_supervised/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RzY48inzXL-"
      },
      "source": [
        "Para construir nuestro clasificador de texto, vamos a seguir un proceso, llamado [ULMFit](https://arxiv.org/abs/1801.06146) que consta de tres pasos representados gráficamente en la siguiente figura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOhxDPLSzXL_"
      },
      "source": [
        "<img alt=\"Diagram of the ULMFiT process\" width=\"700\" caption=\"The ULMFiT process\" id=\"ulmfit_process\" src=\"https://github.com/joheras/fastbook/blob/master/images/att_00027.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm2AHqMmzXMB"
      },
      "source": [
        "En primer lugar es necesario disponer de un modelo de lenguaje entrenado con un dataset de tamaño considerable. En nuestro caso la Wikipedia. Este modelo sirve para conocer los fundamentos del lenguaje con el cual se está trabajando. Sin embargo, a la hora de construir un modelo de clasificación es conveniente que el modelo comprenda el estilo que se usa para escribir esos textos. Dicho estilo puede ser más informal o más técnico que el contenido de la wikipedia. En el caso del dataset IMDb, este va a contener una gran cantidad de nombres de directores, actores, y además el estilo de redacción es más informal que los textos que aparecen en la Wikipedia. Por ello, a partir del modelo de lenguaje de Wikipedia entrenaremos un modelo de lenguaje para IMDb, y a partir de ese modelo de lenguaje construiremos nuestro clasificador.\n",
        "\n",
        "Afortunadamente la librería FastAI ya proporciona un modelo de lenguaje para la Wikipedia (construir este tipo de modelos puede llevar días), por lo que nos podemos centrar en los otros dos pasos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSC7iaCheZKa"
      },
      "source": [
        "## Instalación\n",
        "\n",
        "Comenzamos instalando las librerías necesarias para este notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojCWaLV0eZKa"
      },
      "source": [
        "!pip install fastai --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqD88UV1zXMC"
      },
      "source": [
        "## Modelo de lenguaje para IMDb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqtbKxT8zXME"
      },
      "source": [
        "Antes de crear nuestro modelo de lenguaje para el dataset de IMDB (o para cualquier otro dataset), tenemos que procesarlo. Este proceso va a constar de 4 pasos:\n",
        "\n",
        "1. **Tokenización**: convertir el texto en una lista de palabras (o caractéres o n-gramas dependiendo del nivel de granularidad que se quiera obtener).\n",
        "2. **Numericalización**: construir una lista de todas las palabras que aparecen en el dataset (el vocabulario), y convertir cada palabra en un número mirando el índice que ocupa en el vocabulario. Esta representación es una alternativa al one-hot encoding que vimos en clase. \n",
        "3. **Organizar el dataset**: para entrenar un modelo debemos definir cuáles van a ser nuestras entradas y cuales nuestras salidas. Para este tipo de modelos la entrada es una palabra del texto, y la salida es la siguiente palabra del texto. Por ello, nuestra \"X\" va a ser la secuencia de texto desde la primera palabra hasta la penúltima, y la \"Y\" será la secuencia de texto desde la segunda palabra hasta la última.\n",
        "4. **Construir el modelo de lenguaje**: para ello usaremos una red recurrente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsWSWEm1zXMH"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYUFe6fxzXMI"
      },
      "source": [
        "A la hora de partir un texto en una lista de palabras hay muchas cuestiones por decidir. Por ejemplo, ¿qué hacemos con la puntuación? ¿Cómo tratamos con contracciones como \"don't\"? ¿Qué hacemos con las palabras con guiones? Además en ciertos lenguajes como el alemán o el polaco hay palabras muy largas construidas a partir de muchas piezas, y en idiomas como el japones o el chino no hay un concepto de *palabra*. \n",
        "\n",
        "Por lo tanto, existen distintas aproximaciones para tokenizar un texto que ya vimos en clase. \n",
        "\n",
        "- **Basado en palabras**: se parte el texto basándose en los espacios, y aplicando reglas específicas del lenguaje para separar palabras como \"don't\" en \"do n't\". En general, los signos de puntuación también se separan en tokens. \n",
        "- **Basado en subpalabras**: se parten las palabras en partes más pequeñas basándose en las partes que aparecen más frecuentemente. Por ejemplo, \"occasion\" se puede tokenizar en \"o c ca sion\".\n",
        "- **Basada en caracteres**: se parte una frase por sus caracteres individuales. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UGFvkyxzXMK"
      },
      "source": [
        "### Tokenización con fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfTAGKZTzXML"
      },
      "source": [
        "La librería FastAI ofrece su propio tokenizador, vamos a ver cómo usarlo. Pero antes, tenemos que descargar el dataset de IMDB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vKXDqXuzXMM"
      },
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuoIUgE4eZK1"
      },
      "source": [
        "Con el siguiente comando podemos ver lo que se ha descargado con el comando anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDtqHe1leZK2"
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76BNmQ7aeZK9"
      },
      "source": [
        "Como podemos ver se han descargado varias carpetas, de las cuales a nosotros nos van a interesar tres: la carpeta *train* que va a contenter nuestro conjunto de entrenamiento para construir nuestro clasificador, la carpeta *test* que contiene el conjunto de test, y la carpeta *unsup* que contiene valoraciones que no han sido clasificadas. \n",
        "\n",
        "Podemos ver además que la carpeta *train* (y lo mismo ocurre para la carpeta *test*) contienen dos subcarpetas, una con las valoraciones positivas y otra con las valoraciones negativas.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhXnjjjheZK-"
      },
      "source": [
        "(path/'train').ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmE4N7IpeZLE"
      },
      "source": [
        "Por último cada una de esas subcarpetas contiene un conjunto de ficheros, y cada fichero contiene una valoración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJoQUPXJeZLF"
      },
      "source": [
        "(path/'train'/'pos').ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEzDBTwbzXMS"
      },
      "source": [
        "Por el momento, para constrir nuestro modelo de lenguaje no es necesario saber si una valoración es positiva o si es negativa, y además nos interesa tener un vocabulario lo más amplio posible, por lo tanto para este modelo de lenguaje vamos a cargar los ficheros que aparecen en las carpetas de *train*, *test* y *unsup*. Para ello vamos a usar la función ``get_text_files`` que carga todos los ficheros que aparecen en una carpeta (incluyendo los que aparecen en subcarpetas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhGOicMkzXMT"
      },
      "source": [
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MZzwpNxzXMZ"
      },
      "source": [
        "Ahora podemos mostrar uno de los ficheros que hemos cargado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UEbj0QbzXMa"
      },
      "source": [
        "txt = files[0].open().read(); txt[:75]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di8k38gzzXMh"
      },
      "source": [
        "El proceso de tokenizado en FastAI consta de dos pasos. En primer lugar se aplican una serie de reglas para separar las palabras, esto se hace mediante un `WordTokenizer`, dicho objeto se pasa a la clase `Tokenizer` que aplica una serie de reglas adicionales. Vamos a ver como funcionan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29x9wPefzXMi"
      },
      "source": [
        "wt = WordTokenizer()\n",
        "tkn = Tokenizer(wt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWeYqxGEeZLe"
      },
      "source": [
        "Si aplicamos el `WordTokenizer` a la frase que hemos cargado anteriormente, veremos que el texto se ha separado en una lista de palabras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu6whqnHeZLf"
      },
      "source": [
        "first(wt([txt]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2CybBjazXMv"
      },
      "source": [
        "En cambio si aplicamos el `Tokenizer` obtenemos que no solo se separan las palabras sino que también se pasan a minúsculas y aparecen una serie de tokens que empiezan por los caracteres \"xx\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar7ZeWX_zXMv"
      },
      "source": [
        "coll_repr(tkn(txt), 31)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEhin3zzzXM0"
      },
      "source": [
        "Los tokens que empiezan por los caracteres \"xx\" son tokens especiales. Por ejemplo, el token \"xxbos\" indica el comienzo de un texto. Al reconocer este token de entrada, el modelo será capaz de aprender que necesita \"olvidar\" todo lo que ha aparecido antes y centrarse en las palabras que aparecen a continuación. Existen múltiples reglas de este estilo proporcionadas por fastai que permiten traducir el texto a una lista de tokens que hacen que sea más sencillo el proceso de aprendizaje. Algunos de esos tokens son los siguientes:\n",
        "\n",
        "- xxbos:: comienzo de un texto.\n",
        "- xxmaj:: indica que la siguiente palabra empieza por mayúscula.\n",
        "- xxunk:: indica que la siguiente palabra es desconocida. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XAuhbrBzXNp"
      },
      "source": [
        "Una vez que los textos se han partido en tokens, necesitamos convertirlos a números. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZzKsszSzXNq"
      },
      "source": [
        "### Numericalización con fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBbd4AkzXNr"
      },
      "source": [
        "La numericalización consiste en asignar a cada token un identificador. Para ello, hay que: \n",
        "\n",
        "1. Crear una lista con todas los tokens (el *vocabulario*)\n",
        "1. Reemplazar cada token con su posición en el vocabulario. \n",
        "\n",
        "En la librería FastAI esto se lleva a cabo mediante la clase `Numericalize`. Para llevar a cabo el primer paso debemos constuir nuestro vocabulario (usando las primeras 200 frases de nuestro dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aikPesydzXNw"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])\n",
        "toks200 = txts[:200].map(tkn)\n",
        "num = Numericalize()\n",
        "num.setup(toks200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7otBfuCNzXN0"
      },
      "source": [
        "Podemos ver los primeros 20 elementos de nuestro vocabulario del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "179dnNf4zXN1"
      },
      "source": [
        "coll_repr(num.vocab,20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxCDvf-MzXN6"
      },
      "source": [
        "Notar que los tokens especiales aparecen primero, y a continuación aparecen las palabras de nuestro dataset ordenadas por su frecuencia (es decir, la palabra \"the\" es la que aparece con más frecuencia en nuestro texto). Por defecto, en `Numericalize` el vocabulario puede contener como máximo 60000 palabras (esto puede cambiarse con el atributo `max_vocab`). Esto significa que el resto de palabras se reemplazan por el token de palabra desconocida representado por `xxunk`. Esto evita un uso excesivo de memoria. Del mismo modo las palabras que aparecen con poca frecuencia son reemplazadas con el token `xxunk` (por defecto, una palabra es considerada como \"rara\" si aparece menos de tres veces en el dataset, este valor se puede cambiar con el atributo `min_freqs`).\n",
        "\n",
        "Una vez que se ha creado el objeto `Numericalize` este puede ser usado como una función. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQAMQRzzXN6"
      },
      "source": [
        "toks = tkn(txt)\n",
        "print(coll_repr(tkn(txt), 31))\n",
        "nums = num(toks)[:20]\n",
        "nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4AxJWKnzXN_"
      },
      "source": [
        "También tenemos la operación inversa que convierte los números a palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F0K_otfzXN_"
      },
      "source": [
        "' '.join(num.vocab[o] for o in nums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me8xvnXRzXOD"
      },
      "source": [
        "Ahora que tenemos las palabras representadas como números podemos construir `batches` que serán usados para entrenar nuestro modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZrzHM2PzXOE"
      },
      "source": [
        "### Organizando el dataset en batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW-4BjRBzXOE"
      },
      "source": [
        "Como hemos comentado anteriormente la \"X\" de nuestro dataset va a ser la secuencia de palabras (transformadas a números) de la primera a la penúltima palabra del dataset, y la \"Y\" es la secuencia de palabras desde la segunda hasta la última. Esto se lleva a cabo mediante un objeto de la clase `LMDataLoader` que recibe como parámetro los tokens de nuestro dataset convertidos a números."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txp26SYPzXOV"
      },
      "source": [
        "nums200 = toks200.map(num)\n",
        "dl = LMDataLoader(nums200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKVNP9pzXOc"
      },
      "source": [
        "Además el objeto que acabamos de construir parte el texto en bloques de tamaño 64 que serán usados para entrenar el modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9di2UQ8-zXOc"
      },
      "source": [
        "x,y = first(dl)\n",
        "x.shape,y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRc67OsfzXOg"
      },
      "source": [
        "Podemos ver que el bloque \"X\" contiene una secuencia de tamaño 64, y el bloque \"Y\" la secuencia desplazada un token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDR51GEszXOg"
      },
      "source": [
        "print(' '.join(num.vocab[o] for o in x[0][:20]))\n",
        "print(' '.join(num.vocab[o] for o in y[0][:20]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_-xTDtTzXOl"
      },
      "source": [
        "Todo este proceso que hemos explicado hasta ahora se realiza de manera automática por FastAI cuando construimos un modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyM6D-PeZMe"
      },
      "source": [
        "### Modelo de lenguaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbuzzlSReZMe"
      },
      "source": [
        "Para crear un modelo de lenguaje en FastAI tenemos que:\n",
        "1. Cargar los datos, y \n",
        "2. Entrenar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6M9moXPeZMg"
      },
      "source": [
        "Para cargar los datos usamos la clase `DataBlock` del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-OilFWzXOo"
      },
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyfQzBiWzXOq"
      },
      "source": [
        "Vamos a ver en detalle lo que hace la celda anterior. En primer lugar estamos definiendo una función lambda con el código:\n",
        "\n",
        "``partial(get_text_files, folders=['train', 'test', 'unsup'])``\n",
        "\n",
        "Es decir, el código anterior es equivalente a:\n",
        "\n",
        "``lambda x: get_text_files(x, folders = ['train', 'test', 'unsup'])``\n",
        "\n",
        "A continuación tenemos el ``DataBlock``\n",
        "\n",
        "``DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ")``\n",
        "\n",
        "al que le estamos diciendo que vamos a trabajar con datos de tipo textual (``TextBlock.from_folder(path, is_lm=True)``), que la manera de acceder a los ficheros de texto es a través de la función ``get_imdb`` que hemos definido anteriormente (``get_items=get_imdb``), y que el 10% del dataset se va a usar para evaluar el rendimiento del modelo (`splitter=RandomSplitter(0.1)`). El objeto `DataBlock` tiene además una serie de atributos que es necesario fijar, en concreto le tenemos que indicar el `path` donde están los ficheros, el tamaño del batch (en este caso 128), y la longitud de las secuencias (en este caso 80). \n",
        "\n",
        "Una vez que se han cargado los datos, podemos ver un batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ7CLke_zXOr"
      },
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA78g8NGzXOu"
      },
      "source": [
        "Ahora que los datos están listos podemos entrenar nuestro modelo de lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUfDmi22zXOv"
      },
      "source": [
        "### Fine tuning el modelo de lenguaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b6pw7wlzXOv"
      },
      "source": [
        "El modelo de lenguaje es un modelo RNN que usa una arquitectura llamada *AWD_LSTM*. Para usar este modelo debemos construir un `Learner` del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaP1Dw__zXOw"
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy,Perplexity()]).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-dnNX2LzXOy"
      },
      "source": [
        "En la celda anterior, estamos construyendo nuestro `Learner` al cual le indicamos de donde tiene que obtener los datos (`dls_lm`) y la arquitectura a usar (`AWD_LSTM`). Además le estamos indicando que aplique *Dropout* (`drop_mult=0.3`) y que mida el resultado del modelo usando la *accuracy* y una métrica conocida como *perplexity*. Por último, indicamos que el modelo se tiene que entrenar usando [precisión mixta](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) (a la hora de representar reales) lo que acelera el proceso de entrenamiento. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIfbxBVRzXOz"
      },
      "source": [
        "El proceso que vamos a seguir para entrenar nuestra red, es el mismo que el explicado en el entregable 2. \n",
        "\n",
        "Primero entrenamos solo las últimas capas del modelo (este paso puede llevar cierto tiempo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GF-24cuzXOz"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USvctRZozXO-"
      },
      "source": [
        "Y seguidamente descongelamos la red y la entrenamos entera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H2s8uqPzXO_"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey0iHv3deZM3"
      },
      "source": [
        "Como podéis ver estamos obteniendo una accuracy del 35% esto nos puede parecer bajo, pero daros cuenta que la tarea no es sencilla, estamos intentando adivinar la siguiente palabra de un texto, y hay múltiples posibilades, por lo que un modelo con una accuracy del 35% se puede considerar buena. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMdhpvkgzXPC"
      },
      "source": [
        "Una vez que hemos entrenado el modelo, vamos a guardarlo para crear nuestro modelo de clasificación. Para el modelo de clasificación no va a ser necesaria la última capa del modelo de lenguaje (que es la que se encarga de predecir la siguiente palabra) por lo que vamos a guardar todo el modelo salvo esa última capa, a esto se le conoce como *encoder*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olxqY4__zXPC"
      },
      "source": [
        "learn.save_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6Lyl_szXPG"
      },
      "source": [
        "Con esto ya tenemos nuestro modelo de lenguaje creado, ahora podemos entrenar nuestro modelo de clasificación a partir de el. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdZzbcMzXPO"
      },
      "source": [
        "## Creando un clasificador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQP5G02FzXPP"
      },
      "source": [
        "El proceso para crear un clasificador de texto es muy similar al que hemos seguido para crear un modelo de lenguaje.\n",
        "\n",
        "En primer lugar tenemos que cargar los datos. Esto lo realizamos el siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfd_1v-zXPP"
      },
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7y4VBR9eZNB"
      },
      "source": [
        "Vamos a ver qué significa el código anterior:\n",
        "- `blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock)` sirve para indicar que la entrada de nuesto modelo va a ser texto cargado a partir de los ficheros que se encuentran en path, y que la salida va a ser una clase (es decir que estamos en un problema de clasificación de texto). Notar que al contrario de lo que hacíamos antes, no pasamos el atributo ``is_lm=True`` esto se debe a que ya no estamos construyendo un modelo de lenguaje. Además, proporcionamos el vocabulario que queremos utilizar en lugar de pedir que lo construya. \n",
        "- `get_y = parent_label` sirve para indicar cómo vamos a indicar la etiqueta asociada con cada fichero. Recordar que las carpetas train y test tenían subcarpetas llamadas *pos* y *neg*. Con este código le indicamos que la etiqueta de cada fichero de texto va a ser el nombre de la carpeta que lo contiene. \n",
        "- `get_items=partial(get_text_files, folders=['train', 'test'])` indica de que subcarpetas se van a extraer los ficheros (recordar que nuestro path contiene otras carpetas adicionales).\n",
        "- `splitter=GrandparentSplitter(valid_name='test')` indica que usaremos como conjunto de evaluación los ficheros de la carpeta *test*.\n",
        "- `dataloaders(path, path=path, bs=128, seq_len=72)` configura los distintos parámetros del `DataBlock` de manera análoga a como lo hicimos en el caso del modelo del lenguaje. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6tqnSHrzXPT"
      },
      "source": [
        "Podemos ver uno de los batches que se genera con el anterior `DataBlock` del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCFaxl6ezXPU"
      },
      "source": [
        "dls_clas.show_batch(max_n=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhJ9vAVTzXPd"
      },
      "source": [
        "Ahora pasamos a construir nuestro `Learner` de manera similar a como lo hicimos anteriormente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS3AJfoPzXPd"
      },
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
        "                                metrics=accuracy).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZcWDxuMzXPf"
      },
      "source": [
        "Antes de empezar a entrenarlo, debemos cargar el *encoder* que guardamos anteriormente. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOqsE0wazXPg"
      },
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjJwJcuzXPj"
      },
      "source": [
        "Ahora ya podemos entrenar nuestro clasificador. Comenzamos entrenando solo la última capa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YrvavIAzXPj"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1l4v2n4zXPm"
      },
      "source": [
        "Ahora descongelamos dos capas y volvemos a entrenar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMM8VBDmzXPm"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3va4jtRJzXPo"
      },
      "source": [
        "Podemos descongelar un poco más y seguir entrenando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORlQXSP7zXPp"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz93IpgUzXPr"
      },
      "source": [
        "Y finalmente podemos entrenar el modelo completo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGSL1dsjzXPr"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqHMycPfzXPu"
      },
      "source": [
        "De este modo podemos lograr una accuracy del 94.3% que era el estado del arte hasta hace tres años. Es posible conseguir modelos mejores, pero eso se plantea como posible ampliación. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9IvZTHAenP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}